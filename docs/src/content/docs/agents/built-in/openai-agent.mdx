---
title: Open AI Agent
description: Documentation Open AI Agent
---

The `OpenAIAgent` is a powerful agent class in the Multi-Agent Orchestrator framework that integrates with OpenAI's Chat Completion API. This agent allows you to leverage OpenAI's language models, such as GPT-3.5 and GPT-4, for various natural language processing tasks.

## Key Features

- Integration with OpenAI's Chat Completion API
- Support for multiple OpenAI models
- Streaming and non-streaming response options
- Customizable inference configuration
- Handles conversation history for context-aware responses
- Customizable system prompts
- Support for retrievers to enhance responses with additional context

## Creating an OpenAIAgent

### Minimal Example (Required Fields Only)

Here's the simplest way to create an OpenAIAgent with just the required fields:

import { Tabs, TabItem } from '@astrojs/starlight/components';

<Tabs syncKey="runtime">
  <TabItem label="TypeScript" icon="seti:typescript" color="blue">

```typescript
import { OpenAIAgent } from 'multi-agent-orchestrator';

const agent = new OpenAIAgent({
  name: 'OpenAI Assistant',  // Required
  description: 'A versatile AI assistant', // Required
  apiKey: 'your-openai-api-key' // Required
});
```
  </TabItem>
  <TabItem label="Python" icon="seti:python" color="yellow">

```python
from multi_agent_orchestrator import OpenAIAgent, OpenAIAgentOptions

agent = OpenAIAgent(OpenAIAgentOptions(
    name='OpenAI Assistant',  # Required
    description='A versatile AI assistant',  # Required
    api_key='your-openai-api-key'  # Required
))
```
  </TabItem>
</Tabs>

### Full Example with All Options

Here's an example showing all available configuration options:

<Tabs syncKey="runtime">
  <TabItem label="TypeScript" icon="seti:typescript" color="blue">

```typescript
import { OpenAIAgent } from 'multi-agent-orchestrator';

const agent = new OpenAIAgent({
  // Required fields
  name: 'OpenAI Assistant',
  description: 'A versatile AI assistant powered by OpenAI models',
  apiKey: 'your-openai-api-key',
  
  // Optional fields
  model: 'gpt-3.5-turbo',
  streaming: true,
  retriever: customRetriever, // Custom retriever for additional context
  
  inferenceConfig: {
    maxTokens: 500,
    temperature: 0.7,
    topP: 0.9,
    stopSequences: ['Human:', 'AI:']
  },
  
  customSystemPrompt: {
    template: 'You are a helpful AI assistant specialized in {{DOMAIN}}',
    variables: {
      DOMAIN: 'technology'
    }
  }
});
```
  </TabItem>
  <TabItem label="Python" icon="seti:python" color="yellow">

```python
from multi_agent_orchestrator import OpenAIAgent, OpenAIAgentOptions

agent = OpenAIAgent(OpenAIAgentOptions(
    # Required fields
    name='OpenAI Assistant',
    description='A versatile AI assistant powered by OpenAI models',
    api_key='your-openai-api-key',
    
    # Optional fields
    model='gpt-3.5-turbo',
    streaming=True,
    client=custom_openai_client,  # Custom OpenAI client instance
    retriever=custom_retriever,   # Custom retriever for additional context
    
    inference_config={
        'maxTokens': 500,
        'temperature': 0.7,
        'topP': 0.9,
        'stopSequences': ['Human:', 'AI:']
    },
    
    custom_system_prompt={
        'template': 'You are a helpful AI assistant specialized in {{DOMAIN}}',
        'variables': {
            'DOMAIN': 'technology'
        }
    }
))
```
  </TabItem>
</Tabs>

### OpenAIAgentOptions

The `OpenAIAgentOptions` extends the base `AgentOptions` with the following fields:

<Tabs syncKey="runtime">
<TabItem label="TypeScript" icon="seti:typescript" color="blue">

Required fields:
- `name` (required): Name of the agent
- `description` (required): Description of the agent's capabilities
- `apiKey` (required): Your OpenAI API key for authentication

Optional fields:
- `model` (optional): The OpenAI model identifier (e.g., 'gpt-4', 'gpt-3.5-turbo'). Defaults to `OPENAI_MODEL_ID_GPT_O_MINI`
- `streaming` (optional): Boolean flag for streaming responses. Defaults to `false`
- `retriever` (optional): Custom retriever instance for enhancing responses with additional context
- `inferenceConfig` (optional): Configuration object for model inference:
```typescript
{
  maxTokens?: number;    // Maximum tokens to generate (default: 1000)
  temperature?: number;  // Controls randomness (0-1)
  topP?: number;        // Controls diversity via nucleus sampling
  stopSequences?: string[];  // Sequences that stop generation
}
```
- `customSystemPrompt` (optional): System prompt configuration:
```typescript
{
  template: string;    // The prompt template string
  variables?: {        // Key-value pairs for template variables
    [key: string]: string | string[];
  }
}
```

</TabItem>
<TabItem label="Python" icon="seti:python" color="yellow">

Required fields:
- `name` (required): Name of the agent
- `description` (required): Description of the agent's capabilities
- `api_key` (required): String containing your OpenAI API key for authentication

Optional fields:
- `model` (Optional[str]): The OpenAI model identifier. Defaults to `OPENAI_MODEL_ID_GPT_O_MINI`
- `streaming` (Optional[bool]): Flag for streaming responses. Defaults to `False`
- `client` (Optional[Any]): Custom OpenAI client instance
- `retriever` (Optional[Retriever]): Custom retriever instance for enhancing responses
- `inference_config` (Optional[Dict[str, Any]]): Dictionary configuring model inference:
```python
{
  'maxTokens': int,      # Maximum tokens to generate (default: 1000)
  'temperature': float,  # Controls randomness (0-1)
  'topP': float,        # Controls diversity via nucleus sampling
  'stopSequences': List[str]  # Sequences that stop generation
}
```
- `custom_system_prompt` (Optional[Dict[str, Any]]): Dictionary configuring the system prompt:
```python
{
  'template': str,      # The prompt template string
  'variables': dict     # Key-value pairs for template variables
}
```

</TabItem>
</Tabs>

## Setting the System Prompt

You can set or update the system prompt for the OpenAIAgent in two ways:

1. During initialization:

<Tabs syncKey="runtime">
  <TabItem label="TypeScript" icon="seti:typescript" color="blue">

```typescript
const agent = new OpenAIAgent({
  // ... other options ...
  customSystemPrompt: {
    template: 'You are a helpful AI assistant specialized in answering questions about technology.'
  }
});
```
  </TabItem>
  <TabItem label="Python" icon="seti:python" color="yellow">

```python
agent = OpenAIAgent(OpenAIAgentOptions(
    # ... other options ...
    custom_system_prompt={
        'template': 'You are a helpful AI assistant specialized in answering questions about technology.'
    }
))
```
  </TabItem>
</Tabs>

2. Using the `setSystemPrompt/set_system_prompt` method after initialization:

<Tabs syncKey="runtime">
  <TabItem label="TypeScript" icon="seti:typescript" color="blue">

```typescript
agent.setSystemPrompt(
  `You are an AI assistant specialized in {{DOMAIN}}.
   Your main goal is to {{GOAL}}.
   Always maintain a {{TONE}} tone in your responses.`,
  {
    DOMAIN: "artificial intelligence",
    GOAL: "explain complex AI concepts in simple terms",
    TONE: "friendly and educational"
  }
);
```
  </TabItem>
  <TabItem label="Python" icon="seti:python" color="yellow">

```python
agent.set_system_prompt(
    template="""You are an AI assistant specialized in {{DOMAIN}}.
    Your main goal is to {{GOAL}}.
    Always maintain a {{TONE}} tone in your responses.""",
    variables={
        "DOMAIN": "artificial intelligence",
        "GOAL": "explain complex AI concepts in simple terms",
        "TONE": "friendly and educational"
    }
)
```
  </TabItem>
</Tabs>

## Usage

There are two ways to use the OpenAIAgent: directly or through the Multi-Agent Orchestrator.

### Direct Agent Usage

You can call the agent directly when you want to use a single agent without the orchestrator's routing capabilities:

<Tabs syncKey="runtime">
  <TabItem label="TypeScript" icon="seti:typescript" color="blue">

```typescript
import { OpenAIAgent, ClassifierResult } from 'multi-agent-orchestrator';

const agent = new OpenAIAgent({
  name: 'OpenAI Assistant',
  description: 'A versatile AI assistant',
  apiKey: 'your-openai-api-key'
});

// Create a classifier result for direct agent usage
const classifierResult = {
  selectedAgent: agent,
  confidence: 1.0
};

// Call the agent directly
const response = await orchestrator.agentProcessRequest(
  "What is the capital of France?",
  "user123",
  "session456",
  classifierResult,
  {} // additional parameters (optional)
);

console.log(response.output); // Access the agent's response
console.log(response.metadata); // Access metadata about the request
console.log(response.streaming); // Check if response is streaming
```
  </TabItem>
  <TabItem label="Python" icon="seti:python" color="yellow">

```python
from multi_agent_orchestrator import OpenAIAgent, OpenAIAgentOptions, ClassifierResult

agent = OpenAIAgent(OpenAIAgentOptions(
    name='OpenAI Assistant',
    description='A versatile AI assistant',
    api_key='your-openai-api-key'
))

# Create a classifier result for direct agent usage
classifier_result = ClassifierResult(selected_agent=agent, confidence=1.0)

# Call the agent directly
response = await orchestrator.agent_process_request(
    "What is the capital of France?",
    "user123",
    "session456",
    classifier_result,
    {} # additional parameters (optional)
)

print(response.output)  # Access the agent's response
print(response.metadata)  # Access metadata about the request
print(response.streaming)  # Check if response is streaming
```
  </TabItem>
</Tabs>

### Using through the Orchestrator

When you want to use the agent as part of a multi-agent system, add it to the Multi-Agent Orchestrator:

<Tabs syncKey="runtime">
  <TabItem label="TypeScript" icon="seti:typescript" color="blue">

```typescript
import { MultiAgentOrchestrator } from "multi-agent-orchestrator";

const orchestrator = new MultiAgentOrchestrator();
orchestrator.addAgent(agent);

// The orchestrator will automatically handle agent selection and routing
const response = await orchestrator.routeRequest(
  "What is the capital of France?",
  "user123",
  "session456",
  {} // additional parameters (optional)
);

console.log(response.output); // Access the agent's response
console.log(response.metadata); // Access metadata about the request
console.log(response.streaming); // Check if response is streaming
```
  </TabItem>
  <TabItem label="Python" icon="seti:python" color="yellow">

```python
from multi_agent_orchestrator import MultiAgentOrchestrator

orchestrator = MultiAgentOrchestrator()
orchestrator.add_agent(agent)

# The orchestrator will automatically handle agent selection and routing
response = await orchestrator.route_request(
    "What is the capital of France?",
    "user123",
    "session456",
    {} # additional parameters (optional)
)

print(response.output)  # Access the agent's response
print(response.metadata)  # Access metadata about the request
print(response.streaming)  # Check if response is streaming
```
  </TabItem>
</Tabs>

The key differences between these approaches are:
- Direct usage is simpler when you only need one agent
- Orchestrator usage provides automatic agent selection and handles multiple agents
- Both methods return an `AgentResponse` with the same structure

## Streaming Responses

If you've enabled streaming (`streaming: true` in the options), the agent will return an AsyncIterable that you can use to process the response in chunks:

<Tabs syncKey="runtime">
  <TabItem label="TypeScript" icon="seti:typescript" color="blue">

```typescript
const streamingResponse = await orchestrator.routeRequest(
  "Tell me a long story about a brave knight",
  "user123",
  "session456"
);

if (Symbol.asyncIterator in streamingResponse) {
  for await (const chunk of streamingResponse) {
    console.log(chunk); // Process each chunk of the response
  }
}
```
  </TabItem>
  <TabItem label="Python" icon="seti:python" color="yellow">

```python
streaming_response = await orchestrator.route_request(
    "Tell me a long story about a brave knight",
    "user123",
    "session456"
)

# Check if the response is streaming
if hasattr(streaming_response, '__aiter__'):
    async for chunk in streaming_response:
        print(chunk, end='', flush=True)  # Process each chunk of the response
```
  </TabItem>
</Tabs>

## Using Retrievers

The OpenAIAgent supports retrievers to enhance responses with additional context. Here's how to use a retriever:

<Tabs syncKey="runtime">
  <TabItem label="TypeScript" icon="seti:typescript" color="blue">

```typescript
import { OpenAIAgent, CustomRetriever } from 'multi-agent-orchestrator';

const retriever = new CustomRetriever({
  // Retriever configuration
});

const agent = new OpenAIAgent({
  name: 'OpenAI Assistant',
  description: 'Context-aware AI assistant',
  apiKey: 'your-openai-api-key',
  retriever: retriever
});
```
  </TabItem>
  <TabItem label="Python" icon="seti:python" color="yellow">

```python
from multi_agent_orchestrator import OpenAIAgent, OpenAIAgentOptions, CustomRetriever

retriever = CustomRetriever(
    # Retriever configuration
)

agent = OpenAIAgent(OpenAIAgentOptions(
    name='OpenAI Assistant',
    description='Context-aware AI assistant',
    api_key='your-openai-api-key',
    retriever=retriever
))
```
  </TabItem>
</Tabs>

## Best Practices

1. **API Key Security**: Ensure your OpenAI API key is kept secure and not exposed in your codebase.
2. **Model Selection**: Choose an appropriate model based on your use case and performance requirements.
3. **Inference Configuration**: Experiment with different inference parameters to find the best balance between response quality and speed.
4. **Error Handling**: Implement additional error handling in your application to manage potential API failures gracefully.
5. **Rate Limiting**: Be aware of OpenAI's rate limits and implement appropriate throttling if necessary.
6. **System Prompts**: Craft clear and specific system prompts to guide the model's behavior and improve response quality for your use case.

## Implementation Notes

When implementing the OpenAIAgent in your application:

- In Python, use snake_case naming conventions (e.g., `custom_system_prompt`, `set_system_prompt`)
- In TypeScript, use camelCase naming conventions (e.g., `customSystemPrompt`, `setSystemPrompt`)
- Both implementations support async/await patterns for handling responses
- Both versions include callback support for streaming responses
- Error handling patterns are consistent across both implementations

By leveraging the OpenAIAgent, you can create sophisticated, context-aware AI agents capable of handling a wide range of tasks and interactions, all powered by OpenAI's state-of-the-art language models.