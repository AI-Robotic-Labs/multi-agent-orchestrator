---
title: Open AI Agent
description: Documentation Open AI Agent
---

The `OpenAIAgent` is a powerful agent class in the Multi-Agent Orchestrator framework that integrates with OpenAI's Chat Completion API. This agent allows you to leverage OpenAI's language models, such as GPT-3.5 and GPT-4, for various natural language processing tasks.

## Key Features

- Integration with OpenAI's Chat Completion API
- Support for multiple OpenAI models
- Streaming and non-streaming response options
- Customizable inference configuration
- Handles conversation history for context-aware responses
- Customizable system prompts

## Creating an OpenAIAgent

To create a new `OpenAIAgent`, you need to provide an `OpenAIAgentOptions` object. Here's an example of how to create an OpenAIAgent:

import { Tabs, TabItem } from '@astrojs/starlight/components';

<Tabs syncKey="runtime">
  <TabItem label="Typescript"  icon="seti:typescript" color="blue" >

```typescript
import { OpenAIAgent } from 'multi-agent-orchestrator';

const agent = new OpenAIAgent({
  name: 'OpenAI Assistant',
  description: 'A versatile AI assistant powered by OpenAI models',
  apiKey: 'your-openai-api-key',
  model: 'gpt-3.5-turbo',
  streaming: true,
  inferenceConfig: {
    maxTokens: 500,
    temperature: 0.7,
    topP: 0.9,
    stopSequences: ['Human:', 'AI:']
  },
  customSystemPrompt: {
    template: 'You are a helpful AI assistant specialized in {{DOMAIN}}',
    variables: {
      DOMAIN: 'technology'
    }
  }
});
```
  </TabItem>
  <TabItem label="Python"  icon="seti:python" color="yellow" >

```python
from multi_agent_orchestrator import OpenAIAgent, OpenAIAgentOptions

agent = OpenAIAgent(OpenAIAgentOptions(
    name='OpenAI Assistant',
    description='A versatile AI assistant powered by OpenAI models',
    api_key='your-openai-api-key',
    model='gpt-3.5-turbo',
    streaming=True,
    inference_config={
        'maxTokens': 500,
        'temperature': 0.7,
        'topP': 0.9,
        'stopSequences': ['Human:', 'AI:']
    },
    custom_system_prompt={
        'template': 'You are a helpful AI assistant specialized in {{DOMAIN}}',
        'variables': {
            'DOMAIN': 'technology'
        }
    }
))
```
  </TabItem>
</Tabs>

### OpenAIAgentOptions

The `OpenAIAgentOptions` extends the base `AgentOptions` to provide configuration for the OpenAI agent:

<Tabs syncKey="runtime">
<TabItem label="Typescript" icon="seti:typescript" color="blue">

The TypeScript interface extends `AgentOptions` with the following fields:

- `apiKey` (required): Your OpenAI API key for authentication.
- `model` (optional): The OpenAI model identifier to use (e.g., 'gpt-4', 'gpt-3.5-turbo'). Defaults to `OPENAI_MODEL_ID_GPT_O_MINI`.
- `streaming` (optional): Boolean flag to enable/disable streaming responses. Defaults to `false`.
- `inferenceConfig` (optional): Configuration object for the model's inference behavior:
```typescript
{
  maxTokens?: number;    // Maximum tokens to generate (default: 1000)
  temperature?: number;  // Controls randomness (0-1)
  topP?: number;        // Controls diversity via nucleus sampling
  stopSequences?: string[];  // Sequences that stop generation
}
```
- `customSystemPrompt` (optional): System prompt configuration:
```typescript
{
  template: string;    // The prompt template string
  variables?: {        // Key-value pairs for template variables
    [key: string]: string | string[];
  }
}
```

</TabItem>
<TabItem label="Python" icon="seti:python" color="yellow">

The Python class extends `AgentOptions` with the following fields:

- `api_key` (required): String containing your OpenAI API key for authentication.
- `model` (Optional[str]): The OpenAI model identifier to use (e.g., 'gpt-4', 'gpt-3.5-turbo'). Defaults to `OPENAI_MODEL_ID_GPT_O_MINI`.
- `streaming` (Optional[bool]): Flag to enable/disable streaming responses. Defaults to `False`.
- `inference_config` (Optional[Dict[str, Any]]): Dictionary configuring the model's inference behavior:
```python
{
  'maxTokens': int,      # Maximum tokens to generate (default: 1000)
  'temperature': float,  # Controls randomness (0-1)
  'topP': float,        # Controls diversity via nucleus sampling
  'stopSequences': List[str]  # Sequences that stop generation
}
```
- `custom_system_prompt` (Optional[Dict[str, Any]]): Dictionary configuring the system prompt:
```python
{
  'template': str,      # The prompt template string
  'variables': dict     # Key-value pairs for template variables
}
```

</TabItem>
</Tabs>

## Setting the System Prompt

You can set or update the system prompt for the OpenAIAgent in two ways:

1. During initialization:

<Tabs syncKey="runtime">
  <TabItem label="Typescript"  icon="seti:typescript" color="blue" >

```typescript
const agent = new OpenAIAgent({
  // ... other options ...
  customSystemPrompt: {
    template: 'You are a helpful AI assistant specialized in answering questions about technology.'
  }
});
```
  </TabItem>
  <TabItem label="Python"  icon="seti:python" color="yellow" >

```python
agent = OpenAIAgent(OpenAIAgentOptions(
    # ... other options ...
    custom_system_prompt={
        'template': 'You are a helpful AI assistant specialized in answering questions about technology.'
    }
))
```
  </TabItem>
</Tabs>

2. Using the `setSystemPrompt/set_system_prompt` method after initialization:

<Tabs syncKey="runtime">
  <TabItem label="Typescript"  icon="seti:typescript" color="blue" >

```typescript
agent.setSystemPrompt(
  `You are an AI assistant specialized in {{DOMAIN}}.
   Your main goal is to {{GOAL}}.
   Always maintain a {{TONE}} tone in your responses.`,
  {
    DOMAIN: "artificial intelligence",
    GOAL: "explain complex AI concepts in simple terms",
    TONE: "friendly and educational"
  }
);
```
  </TabItem>
  <TabItem label="Python"  icon="seti:python" color="yellow" >

```python
agent.set_system_prompt(
    template="""You are an AI assistant specialized in {{DOMAIN}}.
    Your main goal is to {{GOAL}}.
    Always maintain a {{TONE}} tone in your responses.""",
    variables={
        "DOMAIN": "artificial intelligence",
        "GOAL": "explain complex AI concepts in simple terms",
        "TONE": "friendly and educational"
    }
)
```
  </TabItem>
</Tabs>

The `setSystemPrompt/set_system_prompt` method allows you to dynamically change the agent's behavior and focus without creating a new instance. You can use placeholders in the prompt template and provide values for them in the second argument.

## Usage

Once you've created an OpenAIAgent, you can add it to the Multi-Agent Orchestrator and use it to process requests:

<Tabs syncKey="runtime">
  <TabItem label="Typescript"  icon="seti:typescript" color="blue" >

```typescript
import { MultiAgentOrchestrator } from "multi-agent-orchestrator";

const orchestrator = new MultiAgentOrchestrator();
orchestrator.addAgent(agent);

const response = await orchestrator.routeRequest(
  "What is the capital of France?",
  "user123",
  "session456"
);
```
  </TabItem>
  <TabItem label="Python"  icon="seti:python" color="yellow" >

```python
from multi_agent_orchestrator import MultiAgentOrchestrator

orchestrator = MultiAgentOrchestrator()
orchestrator.add_agent(agent)

response = await orchestrator.route_request(
    "What is the capital of France?",
    "user123",
    "session456"
)
```
  </TabItem>
</Tabs>

## Streaming Responses

If you've enabled streaming (`streaming: true` in the options), the agent will return an AsyncIterable that you can use to process the response in chunks:

<Tabs syncKey="runtime">
  <TabItem label="Typescript"  icon="seti:typescript" color="blue" >

```typescript
const streamingResponse = await orchestrator.routeRequest(
  "Tell me a long story about a brave knight",
  "user123",
  "session456"
);

if (Symbol.asyncIterator in streamingResponse) {
  for await (const chunk of streamingResponse) {
    console.log(chunk); // Process each chunk of the response
  }
}
```
  </TabItem>
  <TabItem label="Python"  icon="seti:python" color="yellow" >

```python
streaming_response = await orchestrator.route_request(
    "Tell me a long story about a brave knight",
    "user123",
    "session456"
)

# Check if the response is streaming
if hasattr(streaming_response, '__aiter__'):
    async for chunk in streaming_response:
        print(chunk, end='', flush=True)  # Process each chunk of the response
```
  </TabItem>
</Tabs>

## Best Practices

1. **API Key Security**: Ensure your OpenAI API key is kept secure and not exposed in your codebase.
2. **Model Selection**: Choose an appropriate model based on your use case and performance requirements.
3. **Inference Configuration**: Experiment with different inference parameters to find the best balance between response quality and speed.
4. **Error Handling**: Implement additional error handling in your application to manage potential API failures gracefully.
5. **Rate Limiting**: Be aware of OpenAI's rate limits and implement appropriate throttling if necessary.
6. **System Prompts**: Craft clear and specific system prompts to guide the model's behavior and improve response quality for your use case.

## Implementation Notes

When implementing the OpenAIAgent in your application:

- In Python, use snake_case naming conventions (e.g., `custom_system_prompt`, `set_system_prompt`)
- In TypeScript, use camelCase naming conventions (e.g., `customSystemPrompt`, `setSystemPrompt`)
- Both implementations support async/await patterns for handling responses
- Both versions include callback support for streaming responses
- Error handling patterns are consistent across both implementations

By leveraging the OpenAIAgent, you can create sophisticated, context-aware AI agents capable of handling a wide range of tasks and interactions, all powered by OpenAI's state-of-the-art language models.